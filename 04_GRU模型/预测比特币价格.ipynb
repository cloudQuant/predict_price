{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eaf8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential  \n",
    "\n",
    "from tensorflow.keras.optimizers import SGD  \n",
    "\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, GRU  \n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "import numpy as np  \n",
    "\n",
    "import pandas as pd  \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66bff665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date     Open     High      Low\n",
      "0  25-Mar-22  44013.0  45112.0  43622.0\n",
      "1  24-Mar-22  42911.0  44251.0  42658.0\n",
      "2  23-Mar-22  42373.0  43027.0  41795.0\n",
      "3  22-Mar-22  41022.0  43327.9  40893.0\n",
      "4  21-Mar-22  41282.0  41532.0  40530.0\n",
      "     Price\n",
      "0  44331.0\n",
      "1  44013.0\n",
      "2  42912.0\n",
      "3  42373.0\n",
      "4  41022.0\n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件中的数据  \n",
    "\n",
    "data = pd.read_csv(\"BTC_DATA_V3.0.csv\")  \n",
    "\n",
    "# 选取除“Price”和“Vol.”列外的前六列  \n",
    "\n",
    "data = data.iloc[:, 0:6]  \n",
    "\n",
    "# 将“Price”列作为目标变量y  \n",
    "\n",
    "y = data.loc[:, ['Price']]  \n",
    "\n",
    "# 从原始数据中删除“Price”和“Vol.”列  \n",
    "\n",
    "data = data.drop(['Price', 'Vol.'], axis='columns')  \n",
    " \n",
    "\n",
    "# 打印处理后的数据的前五行  \n",
    "\n",
    "print(data.head(5))  \n",
    "\n",
    "# 打印目标变量y的前五行  \n",
    "\n",
    "print(y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "712d4777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2022-03-25', '2022-03-24', '2022-03-23', '2022-03-22',\n",
      "               '2022-03-21', '2022-03-20', '2022-03-19', '2022-03-18',\n",
      "               '2022-03-17', '2022-03-16',\n",
      "               ...\n",
      "               '2018-03-07', '2018-03-06', '2018-03-05', '2018-03-04',\n",
      "               '2018-03-03', '2018-03-02', '2018-03-01', '2018-02-28',\n",
      "               '2018-02-27', '2018-02-26'],\n",
      "              dtype='datetime64[ns]', name='Date', length=1489, freq=None)\n"
     ]
    }
   ],
   "source": [
    "data = data.set_index('Date')  # 将'Date'列设置为数据框的索引  \n",
    "\n",
    "data.index = pd.to_datetime(data.index, unit='ns')  # 将索引转换为datetime64[ns]类型  \n",
    "\n",
    "print(data.index)  # 打印转换后的日期索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5dff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aim = 'Price'  # 设定目标变量为'Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c905323d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1489, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape  # 查询数据框的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "334816c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Price\n",
      "0    44331.0\n",
      "1    44013.0\n",
      "2    42912.0\n",
      "3    42373.0\n",
      "4    41022.0\n",
      "..       ...\n",
      "295  39187.3\n",
      "296  37555.8\n",
      "297  36687.6\n",
      "298  37298.6\n",
      "299  35652.8\n",
      "\n",
      "[300 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train = data[300:]  # 将数据框从第300行开始到末尾作为训练集特征  \n",
    "\n",
    "X_test = data[:300]   # 将数据框从开始到第300行（不包括第300行）作为测试集特征  \n",
    "\n",
    "y_train = y[300:]     # 将目标变量从第300行开始到末尾作为训练集标签  \n",
    "\n",
    "y_test = y[:300]      # 将目标变量从开始到第300行（不包括第300行）作为测试集标签  \n",
    "\n",
    "print(y_test)         # 打印测试集标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759da12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_zero_base(continuous): return continuous / continuous.iloc[0] - 1  \n",
    "\n",
    "def normalise_min_max(continuous): return (continuous - continuous.min()) / (data.max() - continuous.min())\n",
    "# normalise_zero_base 函数将连续数据以第一行数据为基准进行归一化，即除以第一行数据然后减去1。\n",
    "#normalise_min_max 函数将连续数据缩放到0到1之间，通过减去最小值并除以数据范围（最大值减去最小值）实现。但注意这里使用了全局的 data.max() 而不是 continuous.max()，这可能是一个错误，因为通常我们会希望针对 continuous 自身的最大值和最小值进行归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f39eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalise_zero_base(X_train)  \n",
    "\n",
    "X_test = normalise_zero_base(X_test)  \n",
    "\n",
    "y_train = normalise_zero_base(y_train)  \n",
    "\n",
    "y_test = normalise_zero_base(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be49f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)  \n",
    "\n",
    "X_test = np.expand_dims(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a69dcc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras  # 从 TensorFlow 导入 Keras 模块  \n",
    "\n",
    "from tensorflow.keras.layers import GRU, Dropout, PReLU\n",
    "# 定义 GRU 模型  \n",
    "\n",
    "gruMODEL = keras.Sequential()  # 创建一个顺序模型  \n",
    "\n",
    "  \n",
    "\n",
    "# 添加第一个 GRU 层，并应用了 Dropout 正则化  \n",
    "\n",
    "gruMODEL.add(keras.layers.GRU(  \n",
    "\n",
    "    units=1024,  # 隐藏层单元数为 1024  \n",
    "\n",
    "    input_shape=(1, 3),  # 输入形状为 (时间步长, 特征数)，这里时间步长为 1，特征数为 3  \n",
    "\n",
    "    #activation='PReLU',  # 激活函数为 PReLU（参数化修正线性单元） \n",
    "    activation = \"relu\",\n",
    "\n",
    "    recurrent_activation=\"sigmoid\",  # 循环层激活函数为 sigmoid  \n",
    "\n",
    "    use_bias=True,  # 使用偏置项  \n",
    "\n",
    "    kernel_initializer=\"glorot_uniform\",  # 权重初始化方法为 Glorot 均匀分布  \n",
    "\n",
    "    recurrent_initializer=\"orthogonal\",  # 循环层权重初始化方法为正交矩阵  \n",
    "\n",
    "    bias_initializer=\"zeros\",  # 偏置项初始化方法为 0  \n",
    "    # 以下均为正则化和约束选项，此处未使用  \n",
    "\n",
    "    kernel_regularizer=None,  \n",
    "\n",
    "    recurrent_regularizer=None,  \n",
    "\n",
    "    bias_regularizer=None,  \n",
    "\n",
    "    activity_regularizer=None,  \n",
    "\n",
    "    kernel_constraint=None,  \n",
    "\n",
    "    recurrent_constraint=None,  \n",
    "\n",
    "    bias_constraint=None,  \n",
    "\n",
    "    dropout=0.0,  # 在输入层上丢弃的输入单元比例，此处未使用  \n",
    "\n",
    "    recurrent_dropout=0.0,  # 在循环层上丢弃的单元比例，此处未使用  \n",
    "\n",
    "    return_sequences=False,  # 是否返回完整序列的输出，这里只返回最后一个时间步的输出  \n",
    "\n",
    "    return_state=False,  # 是否返回最后一个时间步的状态，此处不返回  \n",
    "\n",
    "    go_backwards=False,  # 是否反向处理输入序列  \n",
    "\n",
    "    stateful=False,  # 如果为 True，则批次中的每个样本的索引 i 的状态将被用作下一个批次中索引 i 的样本的初始状态  \n",
    "\n",
    "    unroll=False,  # 如果为 True，则网络将展开，否则使用符号循环  \n",
    "\n",
    "    # time_major=False,  # 输入和输出的形状格式，False 表示 (batch, time, features)，True 表示 (time, batch, features)  \n",
    "\n",
    "    reset_after=True  # 是否在每个时间步重置门控单元  \n",
    "\n",
    "))  \n",
    "\n",
    "  \n",
    "\n",
    "# 添加 Dropout 层，丢弃率为 0.9 \n",
    "# 添加 PReLU 激活函数作为独立层\n",
    "gruMODEL.add(PReLU())\n",
    "\n",
    "gruMODEL.add(keras.layers.Dropout(0.9))\n",
    "gruMODEL.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee34aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.5346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a9eee183d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gruMODEL.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b1f508c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'inverse_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 进行预测\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# predictions = gruMODEL.predict(X_test)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m gruMODEL\u001b[38;5;241m.\u001b[39minverse_transform(gruMODEL\u001b[38;5;241m.\u001b[39mpredict(X_test))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 打印预测结果\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'inverse_transform'"
     ]
    }
   ],
   "source": [
    "# 进行预测\n",
    "# predictions = gruMODEL.predict(X_test)\n",
    "predictions = gruMODEL.inverse_transform(gruMODEL.predict(X_test))\n",
    "# 打印预测结果\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42ed21e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1024)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fde80465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.032009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.044168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.074643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-0.116029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.152832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>-0.172417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.158634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.195759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Price\n",
       "0    0.000000\n",
       "1   -0.007173\n",
       "2   -0.032009\n",
       "3   -0.044168\n",
       "4   -0.074643\n",
       "..        ...\n",
       "295 -0.116029\n",
       "296 -0.152832\n",
       "297 -0.172417\n",
       "298 -0.158634\n",
       "299 -0.195759\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f15b934a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"gru_8\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 66\u001b[0m\n\u001b[0;32m     60\u001b[0m gruMODEL\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.9\u001b[39m))  \u001b[38;5;66;03m# 添加 Dropout 层，丢弃率为 0.9  \u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 第二层 GRU  \u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m gruMODEL\u001b[38;5;241m.\u001b[39madd(GRU(  \n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m     units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,  \u001b[38;5;66;03m# 隐藏单元数量  \u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# activation='PReLU',  # 激活函数  \u001b[39;00m\n\u001b[0;32m     71\u001b[0m     activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m     recurrent_activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 循环层激活函数  \u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m     use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 是否使用偏置项  \u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m     kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglorot_uniform\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 权重初始化方法  \u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m     recurrent_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morthogonal\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 循环权重初始化方法  \u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m     bias_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 偏置项初始化方法  \u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# 正则化、约束等参数均设置为 None，即不使用  \u001b[39;00m\n\u001b[0;32m     84\u001b[0m \n\u001b[0;32m     85\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,  \u001b[38;5;66;03m# 在每个时间步的输入之间丢弃一部分单元  \u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m     recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,  \u001b[38;5;66;03m# 在每个时间步的循环层之间丢弃一部分单元  \u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[0;32m     89\u001b[0m     return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# 是否返回输出序列的最后一个时间步  \u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m     return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# 是否返回最后一个时间步的状态  \u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m     go_backwards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# 是否反向处理输入序列  \u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m     stateful\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# 如果为 True，则批次中的每个样本将具有其自己的内部状态  \u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[0;32m     97\u001b[0m     unroll\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# 如果为 True，则网络将展开其计算图  \u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# time_major=False,  # 输入数据的形状格式  \u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[0;32m    101\u001b[0m     reset_after\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# 在计算输出时，是否重置隐藏状态  \u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m ))  \n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# 添加 PReLU 激活函数作为独立层\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# gruMODEL.add(PReLU())\u001b[39;00m\n\u001b[0;32m    107\u001b[0m gruMODEL\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.8\u001b[39m))  \u001b[38;5;66;03m# 添加 Dropout 层，丢弃率为 0.8\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\sequential.py:121\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer, rebuild)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_rebuild()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\sequential.py:140\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    139\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(input_shape)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# We can build the Sequential model if the first layer has the\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# model.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput_shape\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:226\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[0;32m    225\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[1;32m--> 226\u001b[0m     original_build_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\sequential.py:186\u001b[0m, in \u001b[0;36mSequential.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:186\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mallow_last_axis_squeeze:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m>\u001b[39m spec\u001b[38;5;241m.\u001b[39mmax_ndim:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"gru_8\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 1024)"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras  \n",
    "\n",
    "from tensorflow.keras.models import Sequential  \n",
    "\n",
    "from tensorflow.keras.layers import GRU, Dropout, PReLU\n",
    "\n",
    "  \n",
    "\n",
    "# 初始化 GRU 模型  \n",
    "\n",
    "gruMODEL = Sequential()  \n",
    "\n",
    "  \n",
    "\n",
    "# 第一层 GRU，并添加 Dropout 正则化  \n",
    "\n",
    "gruMODEL.add(GRU(  \n",
    "\n",
    "    units=1024,  # 隐藏单元数量  \n",
    "\n",
    "    input_shape=(1, 3),  # 输入数据的形状  \n",
    "\n",
    "    # activation='PReLU',  # 激活函数  \n",
    "    activation = \"relu\",\n",
    "\n",
    "    recurrent_activation=\"sigmoid\",  # 循环层激活函数  \n",
    "\n",
    "    use_bias=True,  # 是否使用偏置项  \n",
    "\n",
    "    kernel_initializer=\"glorot_uniform\",  # 权重初始化方法  \n",
    "\n",
    "    recurrent_initializer=\"orthogonal\",  # 循环权重初始化方法  \n",
    "\n",
    "    bias_initializer=\"zeros\",  # 偏置项初始化方法  \n",
    "\n",
    "    # 正则化、约束等参数均设置为 None，即不使用  \n",
    "\n",
    "    dropout=0.0,  # 在每个时间步的输入之间丢弃一部分单元  \n",
    "\n",
    "    recurrent_dropout=0.0,  # 在每个时间步的循环层之间丢弃一部分单元  \n",
    "\n",
    "    return_sequences=False,  # 是否返回输出序列的最后一个时间步  \n",
    "\n",
    "    return_state=False,  # 是否返回最后一个时间步的状态  \n",
    "\n",
    "    go_backwards=False,  # 是否反向处理输入序列  \n",
    "\n",
    "    stateful=False,  # 如果为 True，则批次中的每个样本将具有其自己的内部状态  \n",
    "\n",
    "    unroll=False,  # 如果为 True，则网络将展开其计算图，这可能会消耗更多的内存但会加速计算  \n",
    "\n",
    "    #time_major=False,  # 输入数据的形状格式，此处为（batch, time, features）  \n",
    "\n",
    "    reset_after=True  # 在计算输出时，是否重置隐藏状态  \n",
    "\n",
    "))  \n",
    "# 添加 PReLU 激活函数作为独立层\n",
    "# gruMODEL.add(PReLU())\n",
    "\n",
    "gruMODEL.add(Dropout(0.9))  # 添加 Dropout 层，丢弃率为 0.9  \n",
    "\n",
    "\n",
    "\n",
    "# 第二层 GRU  \n",
    "\n",
    "gruMODEL.add(GRU(  \n",
    "\n",
    "    units=2048,  # 隐藏单元数量  \n",
    "\n",
    "    # activation='PReLU',  # 激活函数  \n",
    "    activation = \"relu\",\n",
    "\n",
    "    recurrent_activation=\"sigmoid\",  # 循环层激活函数  \n",
    "\n",
    "    use_bias=True,  # 是否使用偏置项  \n",
    "\n",
    "    kernel_initializer=\"glorot_uniform\",  # 权重初始化方法  \n",
    "\n",
    "    recurrent_initializer=\"orthogonal\",  # 循环权重初始化方法  \n",
    "\n",
    "    bias_initializer=\"zeros\",  # 偏置项初始化方法  \n",
    "\n",
    "    # 正则化、约束等参数均设置为 None，即不使用  \n",
    "\n",
    "    dropout=0.0,  # 在每个时间步的输入之间丢弃一部分单元  \n",
    "\n",
    "    recurrent_dropout=0.0,  # 在每个时间步的循环层之间丢弃一部分单元  \n",
    "\n",
    "    return_sequences=False,  # 是否返回输出序列的最后一个时间步  \n",
    "\n",
    "    return_state=False,  # 是否返回最后一个时间步的状态  \n",
    "\n",
    "    go_backwards=False,  # 是否反向处理输入序列  \n",
    "\n",
    "    stateful=False,  # 如果为 True，则批次中的每个样本将具有其自己的内部状态  \n",
    "\n",
    "    unroll=False,  # 如果为 True，则网络将展开其计算图  \n",
    "\n",
    "    # time_major=False,  # 输入数据的形状格式  \n",
    "\n",
    "    reset_after=True  # 在计算输出时，是否重置隐藏状态  \n",
    "\n",
    "))  \n",
    "# 添加 PReLU 激活函数作为独立层\n",
    "# gruMODEL.add(PReLU())\n",
    "\n",
    "gruMODEL.add(Dropout(0.8))  # 添加 Dropout 层，丢弃率为 0.8\n",
    "# 在这个模型中，我们首先定义了一个 Sequential 模型，然后添加了两个 GRU 层，每个 GRU 层后面都跟随了一个 Dropout 层。第一层 GRU 的隐藏单元数为 1024，输入数据的形状为 (1, 3)，表示每个样本有一个时间步长和三个特征。第二层 GRU 的隐藏单元数增加到了 2048，并且两个 Dropout 层的丢弃率分别设置为 0.9 和 0.8，用于在训练过程中随机丢弃一部分神经元的输出，以减少过拟合的风险。\n",
    "\n",
    "# 第三层GRU层\n",
    "\n",
    "gruMODEL.add(GRU(  \n",
    "    units=4096, # 神经元数量  \n",
    "    # activation='PReLU', # 激活函数为PReLU  \n",
    "    activation = \"relu\",\n",
    "    recurrent_activation=\"sigmoid\", # 递归激活函数为sigmoid  \n",
    "    use_bias=True, # 使用偏置项  \n",
    "    kernel_initializer=\"glorot_uniform\", # 权重初始化方法  \n",
    "    recurrent_initializer=\"orthogonal\", # 递归权重初始化方法  \n",
    "    bias_initializer=\"zeros\", # 偏置初始化方法  \n",
    "    kernel_regularizer=None, # 权重正则化方法  \n",
    "    recurrent_regularizer=None, # 递归权重正则化方法  \n",
    "    bias_regularizer=None, # 偏置正则化方法  \n",
    "    activity_regularizer=None, # 激活正则化方法  \n",
    "    kernel_constraint=None, # 权重约束  \n",
    "    recurrent_constraint=None, # 递归权重约束  \n",
    "    bias_constraint=None, # 偏置约束  \n",
    "    dropout=0.0, # dropout层比例  \n",
    "    recurrent_dropout=0.0, # 递归dropout层比例  \n",
    "    return_sequences=False, # 是否返回输出序列的最后一个输出  \n",
    "    return_state=False, # 是否返回最后一个状态  \n",
    "    go_backwards=False, # 是否反向计算  \n",
    "    stateful=False, # 是否是有状态的网络  \n",
    "    unroll=False, # 是否展开网络  \n",
    "    # time_major=False, # 输入张量的形状格式  \n",
    "    reset_after=True)) # 是否在每个时间步之后重置门控单元\n",
    "# 添加 PReLU 激活函数作为独立层\n",
    "# gruMODEL.add(PReLU())\n",
    "gruMODEL.add(Dropout(0.09)) # 添加dropout层，防止过拟合\n",
    "\n",
    "# 第四层GRU层\n",
    "\n",
    "gruMODEL.add(GRU(  \n",
    "    units=512, # 神经元数量  \n",
    "    # activation='PReLU', # 激活函数为PReLU  \n",
    "    activation = \"relu\",\n",
    "    recurrent_activation=\"sigmoid\", # 递归激活函数为sigmoid  \n",
    "    use_bias=True, # 使用偏置项  \n",
    "    kernel_initializer=\"glorot_uniform\", # 权重初始化方法  \n",
    "    recurrent_initializer=\"orthogonal\", # 递归权重初始化方法  \n",
    "    bias_initializer=\"zeros\", # 偏置初始化方法  \n",
    "    kernel_regularizer=None, # 权重正则化方法  \n",
    "    recurrent_regularizer=None, # 递归权重正则化方法  \n",
    "    bias_regularizer=None, # 偏置正则化方法  \n",
    "    activity_regularizer=None, # 激活正则化方法  \n",
    "    kernel_constraint=None, # 权重约束  \n",
    "    recurrent_constraint=None, # 递归权重约束  \n",
    "    bias_constraint=None, # 偏置约束  \n",
    "    dropout=0.0, # dropout层比例  \n",
    "    recurrent_dropout=0.0, # 递归dropout层比例  \n",
    "    return_sequences=False, # 是否返回输出序列的最后一个输出  \n",
    "    return_state=False, # 是否返回最后一个状态  \n",
    "    go_backwards=False, # 是否反向计算  \n",
    "    stateful=False, # 是否是有状态的网络  \n",
    "    unroll=False, # 是否展开网络  \n",
    "    # time_major=False, # 输入张量的形状格式  \n",
    "    reset_after=True)) # 是否在每个时间步之后重置门控单元\n",
    "\n",
    "gruMODEL.add(Dropout(0.09)) # 添加dropout层，防止过拟合\n",
    "\n",
    "# 输出层\n",
    "\n",
    "gruMODEL.add(Dense(units=1)) # 添加全连接层，输出单元数为1\n",
    "\n",
    "# 编译RNN\n",
    "\n",
    "gruMODEL.compile(optimizer=\"sgd\", # 使用随机梯度下降优化器  \n",
    "loss='mean_squared_error') # 使用均方误差作为损失函数  \n",
    "gruMODEL.summary() # 打印模型概览信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络层数和每层神经元数量  \n",
    "\n",
    "layers = [3,15,30,45,90,1]  \n",
    "\n",
    "# 定义各层的文本标签  \n",
    "\n",
    "layers_str = [\"Input\"] + [\"GRU\"] * (len(layers) - 2) + [\"Output\"]  \n",
    "\n",
    "# 定义各层的颜色属性（这里实际未使用）  \n",
    "\n",
    "layers_col = [\"none\"] + [\"none\"] * (len(layers) - 2) + [\"none\"]  \n",
    "\n",
    "# 定义各层的填充颜色  \n",
    "\n",
    "layers_fill = [\"black\"] + [\"gray\"] * (len(layers) - 2) + [\"black\"]  \n",
    "\n",
    "# 节点边框宽度  \n",
    "\n",
    "penwidth = 15  \n",
    "\n",
    "# 字体设置  \n",
    "\n",
    "font = \"Hilda 10\"  \n",
    "  \n",
    "\n",
    "# 开始打印DOT语言描述  \n",
    "\n",
    "print(\"digraph G {\")  \n",
    "\n",
    "print(\"\\tfontname = \"{}\"\".format(font))  \n",
    "\n",
    "print(\"\\trankdir=LR\")  # 设置图的布局方向为从左到右  \n",
    "\n",
    "print(\"\\tsplines=line\")  # 设置边为直线  \n",
    "\n",
    "print(\"\\tnodesep=.08;\")  # 设置节点之间的间距  \n",
    "\n",
    "print(\"\\tranksep=1;\")  # 设置层次之间的间距  \n",
    "\n",
    "print(\"\\tedge [color=black, arrowsize=.5];\")  # 设置边的颜色和箭头大小  \n",
    "\n",
    "print(\"\\tnode [fixedsize=true,label=\"\",style=filled,\" + \\  \n",
    "\n",
    "      \"color=none,fillcolor=gray,shape=circle]\\n\")  # 设置节点属性，如固定大小、填充样式和颜色等  \n",
    "\n",
    "  \n",
    "\n",
    "# 绘制各层（子图）  \n",
    "\n",
    "for i in range(0, len(layers)):  \n",
    "\n",
    "    print(\"\\tsubgraph cluster_{} {{\".format(i))  \n",
    "\n",
    "    print(\"\\t\\tcolor={};\".format(layers_col[i]))  # 设置子图颜色（这里实际未使用）  \n",
    "\n",
    "    print(\"\\t\\tnode [style=filled, color=white, penwidth={},\"  \n",
    "\n",
    "          \"fillcolor={} shape=circle];\".format(penwidth, layers_fill[i]))  # 设置子图中节点的属性  \n",
    "\n",
    "    print(\"\\t\\t\", end=' ')  \n",
    "\n",
    "    for a in range(layers[i]):  \n",
    "\n",
    "        print(\"l{}{} \".format(i + 1, a), end=' ')  # 打印节点标签  \n",
    "\n",
    "    print(\";\")  \n",
    "\n",
    "    print(\"\\t\\tlabel = {};\".format(layers_str[i]))  # 设置子图的标签  \n",
    "\n",
    "    print(\"\\t}\\n\")  \n",
    "\n",
    "\n",
    "## 绘制节点间的边  \n",
    "\n",
    "for i in range(1, len(layers)):  \n",
    "\n",
    "    for a in range(layers[i - 1]):  \n",
    "\n",
    "        for b in range(layers[i]):  \n",
    "\n",
    "            print(\"\\tl{}{} -> l{}{}\".format(i, a, i + 1, b))  # 绘制从第i层的第a个节点到第i+1层的第b个节点的边  \n",
    "\n",
    "print(\"}\")  # 结束DOT语言描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c661033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用'w'模式打开名为'model.txt'的文件，准备写入网络层结构信息  \n",
    "\n",
    "with open('model.txt', 'w') as layers_file:  \n",
    "\n",
    "    # 定义网络各层的神经元数量  \n",
    "\n",
    "    layers = [3,5,10,15,20,1]  \n",
    "\n",
    "    # 定义各层的文本标签  \n",
    "\n",
    "    layers_str = [\"Input\"] + [\"GRU\"] * (len(layers) - 2) + [\"Output\"]  \n",
    "\n",
    "    # 定义各层的颜色属性（此处未实际使用）  \n",
    "\n",
    "    layers_col = [\"none\"] + [\"none\"] * (len(layers) - 2) + [\"none\"]  \n",
    "\n",
    "    # 定义各层的填充颜色  \n",
    "\n",
    "    layers_fill = [\"black\"] + [\"gray\"] * (len(layers) - 2) + [\"black\"]  \n",
    "\n",
    "    # 节点边框宽度  \n",
    "\n",
    "    penwidth = 15  \n",
    "\n",
    "    # 字体设置  \n",
    "\n",
    "    font = \"Hilda 10\"  \n",
    "\n",
    "  \n",
    "\n",
    "    # 写入DOT语言描述，用于描述网络结构图  \n",
    "\n",
    "    layers_file.write(\"digraph G {\\n\")  # 开始定义有向图G  \n",
    "\n",
    "    layers_file.write(\"\\tfontname = \"{}\"\\n\".format(font))  # 设置字体名称为Hilda 10  \n",
    "\n",
    "    layers_file.write(\"\\trankdir=LR\\n\")  # 设置图的布局方向为从左到右  \n",
    "\n",
    "    layers_file.write(\"\\tsplines=line\\n\")  # 设置边为直线  \n",
    "\n",
    "    layers_file.write(\"\\tnodesep=.08;\\n\")  # 设置节点之间的间距  \n",
    "\n",
    "    layers_file.write(\"\\tranksep=1;\\n\")  # 设置层次之间的间距  \n",
    "\n",
    "    layers_file.write(\"\\tedge [color=black, arrowsize=.5];\\n\")  # 设置边的颜色和箭头大小  \n",
    "\n",
    "    layers_file.write(\"\\tnode [fixedsize=true,label=\"\",style=filled,color=none,fillcolor=gray,shape=circle]\\n\\n\")  # 设置节点属性  \n",
    "\n",
    "  \n",
    "\n",
    "    # 绘制各层（子图）  \n",
    "\n",
    "    for i in range(0, len(layers)):  \n",
    "\n",
    "        layers_file.write(\"\\tsubgraph cluster_{} {{\\n\".format(i))  # 开始定义第i个子图  \n",
    "\n",
    "        layers_file.write(\"\\t\\tcolor={};\\n\".format(layers_col[i]))  # 设置子图颜色（此处未实际使用）  \n",
    "\n",
    "        layers_file.write(\"\\t\\tnode [style=filled, color=white, penwidth={},fillcolor={} shape=circle];\\n\".format(penwidth, layers_fill[i]))  # 设置子图中节点的属性  \n",
    "\n",
    "        layers_file.write(\"\\t\\t\")  \n",
    "\n",
    "        for a in range(layers[i]):  \n",
    "\n",
    "            layers_file.write(\"l{}{} \".format(i + 1, a))  # 写入节点标签  \n",
    "\n",
    "        layers_file.write(\";\\n\")  \n",
    "\n",
    "        layers_file.write(\"\\t\\tlabel = {};\\n\".format(layers_str[i]))  # 设置子图的标签  \n",
    "\n",
    "        layers_file.write(\"\\t}\\n\\n\")  # 结束定义第i个子图  \n",
    "\n",
    "  \n",
    "\n",
    "    # 绘制节点间的边  \n",
    "\n",
    "    for i in range(1, len(layers)):  \n",
    "\n",
    "        for a in range(layers[i - 1]):  \n",
    "\n",
    "            for b in range(layers[i]):  \n",
    "\n",
    "                layers_file.write(\"\\tl{}{} -> l{}{}\\n\".format(i, a, i + 1, b))  # 绘制从第i层的第a个节点到第i+1层的第b个节点的边  \n",
    "\n",
    "  \n",
    "\n",
    "    layers_file.write(\"}\\n\")  # 结束DOT语言描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score  \n",
    "\n",
    "r2_score = r2_score(y_test, preds)  \n",
    "\n",
    "r2_score * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fab0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array([[44331,44818,44090]])  # 这里假设的预测数据，用于后续计算  \n",
    "\n",
    "X_testt = scaling.inverse_transform(X_test[0])  # 对测试数据进行逆缩放处理  \n",
    "\n",
    "prediction_new = np.array([[(X_test[0][0]/X_testt[0]*prediction[0])]])  # 基于缩放后的数据重新计算预测值  \n",
    "\n",
    "predictions = gruMODEL.predict(prediction_new)[0][0]  # 使用GRU模型进行预测  \n",
    "\n",
    "predictions = np.array([[predictions]]) * prediction[0][0] / prediction_new[0][0][0]  # 对预测结果进行比例调整  \n",
    "\n",
    "f\"27 March Prediction is {predictions[0][0]} BTC/USDT\"  # 输出预测结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
